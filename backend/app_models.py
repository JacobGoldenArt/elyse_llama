from typing import Dict, List, Optional

from litellm import (
    BaseModel as LiteLLMBaseModel,  # Alias to avoid conflict if llama_index.core.BaseModel is used elsewhere
)
from llama_index.core.llms import ChatMessage
from llama_index.core.workflow import Context, Event, StartEvent

"""
This module defines the Pydantic models used throughout the Elyse AI application.
These models serve as data structures for application settings, workflow context, 
events, and input/output schemas, ensuring clear data contracts between components.

Using Pydantic models helps with data validation, serialization, and editor support (e.g., autocompletion).
"""

# Settings and Context Models


class AppSettings(LiteLLMBaseModel):
    """Global application settings, not specific to a single LLM call."""

    tts_enabled: Optional[bool] = False
    """Enable or disable Text-to-Speech functionality."""
    stt_enabled: Optional[bool] = False
    """Enable or disable Speech-to-Text functionality."""
    stt_model: Optional[str] = "google"
    """Specifies the Speech-to-Text model to be used."""
    embedding_model: Optional[str] = "openai"
    """Specifies the embedding model to be used for RAG or other semantic tasks."""
    sfw_mode: Optional[bool] = False
    """Enable or disable Safe-for-Work mode, potentially filtering content."""


class ModelSettings(LiteLLMBaseModel):
    """Settings specific to an individual Language Model (LLM) call."""

    model: Optional[str] = "gpt-4o-mini"
    """The identifier of the LLM to be used (e.g., 'openai/gpt-4o-mini')."""
    temperature: Optional[float] = 1.0
    """Controls randomness in the LLM's output. Higher values mean more randomness."""
    max_tokens: Optional[int] = 1000
    """The maximum number of tokens the LLM should generate in its response."""
    top_p: Optional[float] = 1.0
    """Controls nucleus sampling. The model considers only tokens with top_p probability mass."""
    frequency_penalty: Optional[float] = 0.0
    """Penalizes new tokens based on their existing frequency in the text so far, decreasing repetition."""
    presence_penalty: Optional[float] = 0.0
    """Penalizes new tokens based on whether they appear in the text so far, increasing novelty."""


class WorkflowStartEvent(StartEvent):
    """
    Event that initiates a run of the AppWorkFlow.
    It inherits from LlamaIndex's StartEvent and carries initial data for the workflow.
    """

    user_message: str
    """The message input by the user for the current turn."""
    settings: ModelSettings
    """The LLM settings to be applied for this workflow run."""
    initial_models_to_use: Optional[List[str]] = None
    """A list of LLM identifiers to be used for generating candidate responses."""
    chat_history: Optional[List[ChatMessage]] = None
    """The conversation history up to the previous turn. LlamaIndex's ChatMessage is used for structure."""
    workflow_run_id: Optional[str] = (
        None  # Added for tracking specific workflow runs for async operations
    )
    """An optional unique identifier for this specific workflow run, used for async operations like user feedback."""


class GlobalContext(Context):
    """
    The global context object for the AppWorkFlow, accessible by all workflow steps.
    It holds the state of the workflow as it progresses through its steps.
    Inherits from LlamaIndex's Context.
    """

    user_message: Optional[str] = ""
    """The current user's message being processed."""
    chat_history: Optional[List[ChatMessage]] = []
    """The full conversation history, updated after each turn."""
    context_needed: Optional[bool] = False  # Placeholder for future RAG logic
    """Flag indicating if context retrieval is necessary (for RAG)."""
    context_retrieved: Optional[str] = ""  # Placeholder for future RAG logic
    """The text content retrieved from a knowledge base (for RAG)."""
    app_settings: Optional[AppSettings] = AppSettings()
    """Current application-level settings."""
    model_settings: Optional[ModelSettings] = ModelSettings()
    """LLM settings active for the current workflow run."""
    models_to_use: Optional[List[str]] = []
    """List of LLM identifiers selected for generating responses."""
    response_candidates: Optional[List[str]] = []
    """A list of response strings generated by the different LLMs."""
    current_system_prompt: Optional[str] = ""
    """The system prompt constructed for the current turn."""
    curated_ai_response: Optional[str] = ""
    """The AI response selected by the user during the curation step."""
    workflow_run_id: Optional[str] = (
        None  # Added for tracking specific workflow runs for async operations
    )
    """An optional unique identifier for this specific workflow run, used for async operations like user feedback."""


# Workflow Events
# These models represent the data passed between workflow steps.
# Each event typically signifies the completion of a step and carries its output.


class ProcessInput(Event):
    """Event produced after the initial user input has been processed."""

    first_output: str  # Example field, content may vary based on step's purpose.
    """Output from the input processing step, e.g., a confirmation message."""


class RetrieveContext(Event):
    """Event produced after the context retrieval step (simulated for now)."""

    context_retrieved: str
    """The text content retrieved (or simulated) for RAG purposes."""


class DynamicPromptBuilt(Event):
    """Event produced after the dynamic prompt has been constructed."""

    prompt_updated: (
        str  # Typically would carry the List[ChatMessage] or the system prompt string.
    )
    """A confirmation or snippet of the updated prompt."""


class LlmResponsesCollected(Event):
    """Event produced after all selected LLMs have generated their responses."""

    responses_collected_count: int
    """The number of responses successfully collected from the LLMs."""


class CurationManager(Event):
    """Event produced after the user has curated the LLM responses."""

    curated_response: str
    """The final AI response string chosen by the user."""


# Workflow Output Model
class WorkflowRunOutput(
    LiteLLMBaseModel
):  # Using LiteLLMBaseModel for consistency, can be LlamaIndex Pydantic model too
    """
    Defines the final output structure of a complete AppWorkFlow run.
    This model is returned when the workflow's StopEvent is triggered.
    """

    final_response: str
    """The curated AI response that will be presented to the user."""
    chat_history: List[ChatMessage]
    """The updated chat history, including the latest user message and the curated AI response."""


# --- Events for Streaming ---
# These events are designed to be written to the workflow's event stream (ctx.write_event_to_stream)
# and then relayed via Server-Sent Events (SSE) to the client.


class WorkflowStepUpdateEvent(Event):
    """Signals progress or status update from a specific workflow step."""

    step_name: str
    """The name of the workflow step generating the update."""
    status: str
    """A descriptive status message (e.g., 'started', 'completed', 'fetching data')."""
    data: Optional[Dict] = None
    """Optional dictionary to carry any relevant data for this update."""


class LLMTokenStreamEvent(Event):
    """Event carrying a single token from an LLM's streaming response."""

    model_name: str
    """The name of the model generating this token."""
    token: str
    """The token string."""
    is_final_chunk: bool = False
    """Indicates if this is the last token/chunk for this model's response stream."""


class LLMCandidateReadyEvent(Event):
    """Event indicating a full candidate response from one LLM is ready."""

    model_name: str
    """The name of the model that generated the candidate."""
    candidate_response: str
    """The full text of the candidate response."""


class CurationRequiredEvent(Event):
    """Event indicating that all LLM candidates are ready and user curation is now required."""

    response_candidates: List[
        str
    ]  # Or List[Dict[str,str]] if sending model_name with each
    """A list of all candidate response strings."""
    message: str = "Curation required: Please select the best response."
    """A message for the user."""
    workflow_run_id: str  # Added: This is crucial for the client to send back with the curated choice
    """The unique ID of the workflow run that requires curation, to be sent back by the client."""


class WorkflowErrorEvent(Event):
    """Event to signal an error that occurred during workflow execution."""

    step_name: Optional[str] = None
    """The name of the step where the error occurred, if applicable."""
    error_message: str
    """A description of the error."""
